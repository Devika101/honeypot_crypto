"""WGAN-GP Discriminator for evaluating honeypot infrastructure realism.

Evaluates whether network configurations are real (from actual network scans)
or generated by the Generator. Analyzes:
- Service distribution patterns
- Vulnerability placement realism
- Network topology coherence
- Port/protocol combinations

Uses spectral normalization for training stability.
"""

from __future__ import annotations

from typing import Optional

import torch
import torch.nn as nn
from torch.nn.utils import spectral_norm


class FeatureExtractor(nn.Module):
    """Extracts features from a variable-size tensor with spectral normalization."""

    def __init__(self, in_features: int, out_features: int, num_layers: int = 2) -> None:
        super().__init__()
        layers: list[nn.Module] = []
        dim = in_features
        for _ in range(num_layers - 1):
            mid = (dim + out_features) // 2
            layers.extend([
                spectral_norm(nn.Linear(dim, mid)),
                nn.LeakyReLU(0.2, inplace=True),
                nn.Dropout(0.1),
            ])
            dim = mid
        layers.append(spectral_norm(nn.Linear(dim, out_features)))
        layers.append(nn.LeakyReLU(0.2, inplace=True))
        self.net = nn.Sequential(*layers)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Process input of shape (batch, num_items, feature_dim) -> (batch, out_features)."""
        # Flatten items dimension and pass through network
        batch_size = x.size(0)
        x = x.view(batch_size, -1)
        return self.net(x)


class Discriminator(nn.Module):
    """WGAN-GP Discriminator with spectral normalization.

    Architecture:
        1. Three parallel feature extractors for services, vulnerabilities, topology
        2. Condition embedding
        3. Fusion network combining all features
        4. Critic output (unbounded score, not probability)

    The discriminator returns both the critic score and intermediate feature
    representations (useful for feature matching loss).

    Args:
        service_input_dim: Flattened dimension of service tensor.
        vuln_input_dim: Flattened dimension of vulnerability tensor.
        topology_input_dim: Flattened dimension of topology tensor.
        condition_dim: Dimension of the condition vector.
        feature_dim: Internal feature dimension for each extractor.
        hidden_dims: Hidden dimensions for the fusion network.
    """

    def __init__(
        self,
        service_input_dim: int = 160,    # 20 services * 8 features
        vuln_input_dim: int = 180,       # 30 vulns * 6 features
        topology_input_dim: int = 70,    # 10 subnets * 7 features
        condition_dim: int = 32,
        feature_dim: int = 128,
        hidden_dims: Optional[list[int]] = None,
    ) -> None:
        super().__init__()
        hidden_dims = hidden_dims or [512, 256, 128]

        # Parallel feature extractors
        self.service_extractor = FeatureExtractor(service_input_dim, feature_dim)
        self.vuln_extractor = FeatureExtractor(vuln_input_dim, feature_dim)
        self.topology_extractor = FeatureExtractor(topology_input_dim, feature_dim)

        # Condition embedding
        self.condition_embed = nn.Sequential(
            spectral_norm(nn.Linear(condition_dim, feature_dim)),
            nn.LeakyReLU(0.2, inplace=True),
        )

        # Fusion network: 4 * feature_dim -> critic score
        fusion_input = feature_dim * 4
        fusion_layers: list[nn.Module] = []
        dim = fusion_input
        for h in hidden_dims:
            fusion_layers.extend([
                spectral_norm(nn.Linear(dim, h)),
                nn.LeakyReLU(0.2, inplace=True),
                nn.Dropout(0.1),
            ])
            dim = h

        self.fusion = nn.Sequential(*fusion_layers)

        # Critic head: outputs unbounded score (WGAN style, no sigmoid)
        self.critic_head = spectral_norm(nn.Linear(dim, 1))

        # Store last features for feature matching
        self._last_features: Optional[torch.Tensor] = None

    def forward(
        self,
        services: torch.Tensor,
        vulnerabilities: torch.Tensor,
        topology: torch.Tensor,
        condition: torch.Tensor,
    ) -> torch.Tensor:
        """Evaluate the realism of an infrastructure configuration.

        Args:
            services: (batch, max_services, service_feature_dim)
            vulnerabilities: (batch, max_vulns, vuln_feature_dim)
            topology: (batch, max_subnets, subnet_feature_dim)
            condition: (batch, condition_dim)

        Returns:
            Critic scores of shape (batch, 1). Higher = more realistic.
        """
        svc_feat = self.service_extractor(services)
        vuln_feat = self.vuln_extractor(vulnerabilities)
        topo_feat = self.topology_extractor(topology)
        cond_feat = self.condition_embed(condition)

        combined = torch.cat([svc_feat, vuln_feat, topo_feat, cond_feat], dim=-1)
        features = self.fusion(combined)

        self._last_features = features.detach()

        return self.critic_head(features)

    def get_features(self) -> Optional[torch.Tensor]:
        """Return the last computed intermediate features (for feature matching loss)."""
        return self._last_features

    def gradient_penalty(
        self,
        real_services: torch.Tensor,
        real_vulns: torch.Tensor,
        real_topology: torch.Tensor,
        fake_services: torch.Tensor,
        fake_vulns: torch.Tensor,
        fake_topology: torch.Tensor,
        condition: torch.Tensor,
    ) -> torch.Tensor:
        """Compute WGAN-GP gradient penalty.

        Interpolates between real and fake samples and penalizes the discriminator
        when the gradient norm deviates from 1.

        Returns:
            Scalar gradient penalty loss.
        """
        batch_size = real_services.size(0)
        device = real_services.device

        # Random interpolation coefficient
        eps_s = torch.rand(batch_size, 1, 1, device=device)
        eps_v = torch.rand(batch_size, 1, 1, device=device)
        eps_t = torch.rand(batch_size, 1, 1, device=device)

        # Interpolated samples
        interp_s = (eps_s * real_services + (1 - eps_s) * fake_services).requires_grad_(True)
        interp_v = (eps_v * real_vulns + (1 - eps_v) * fake_vulns).requires_grad_(True)
        interp_t = (eps_t * real_topology + (1 - eps_t) * fake_topology).requires_grad_(True)

        critic_interp = self.forward(interp_s, interp_v, interp_t, condition)

        gradients = torch.autograd.grad(
            outputs=critic_interp,
            inputs=[interp_s, interp_v, interp_t],
            grad_outputs=torch.ones_like(critic_interp),
            create_graph=True,
            retain_graph=True,
        )

        # Concatenate gradients and compute penalty
        grad_cat = torch.cat([g.view(batch_size, -1) for g in gradients], dim=1)
        grad_norm = grad_cat.norm(2, dim=1)
        penalty = ((grad_norm - 1) ** 2).mean()

        return penalty
